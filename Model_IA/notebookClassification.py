# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfEVe7FST2U-smT7wGDbVBJjLZeer4cx

##Examen - Cloud Computing

##### Intel Dataset - Natural Scene

Import Packages
"""

import numpy as np 
import pandas as pd 
import seaborn as sn 
import matplotlib.pyplot as plt 
import tensorflow as tf 
import zipfile
import cv2
import os
import io

from sklearn.utils import shuffle  
from tqdm import tqdm

# Version TensorFlow
print(tf.__version__)

"""Loading Data"""

# Load Train Dataset
from google.colab import files
uploaded = files.upload()

dataTrain = zipfile.ZipFile(io.BytesIO(uploaded['seg_train.zip']), 'r')
dataTrain.extractall()

# Load Test Dataset
from google.colab import files
uploaded = files.upload()

# Load Prediction Dataset
dataTest = zipfile.ZipFile(io.BytesIO(uploaded['seg_test.zip']), 'r')
dataTest.extractall()

from google.colab import files
uploaded = files.upload()

dataPred = zipfile.ZipFile(io.BytesIO(uploaded['seg_pred.zip']), 'r')
dataPred.extractall()

# Define class names:
class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']
class_names_label = {class_name:i for i, class_name in enumerate(class_names)}
nb_class = len(class_names)

def load_data():

  datasetList = ['/content/seg_train', '/content/seg_test']
  output = []
    
  # Iterate through training and test sets
  for dataset in datasetList:
    
    imageL = []
    labelL = []
    print("Loading {}".format(dataset))
    
    # Iterate through each folder corresponding to a category
    for folder in os.listdir(dataset):
      label = class_names_label[folder]

      # Iterate through each image in our folder
      for file in tqdm(os.listdir(os.path.join(dataset,folder))):
        # Get the path name of the image
        img_path = os.path.join(os.path.join(dataset,folder), file)
            
        # Open and resize the img
        image = cv2.imread(img_path)
        image = cv2.resize(image, (150,150) ) 
            
        # Append the image and its corresponding label to the output
        imageL.append(image)
        labelL.append(label)
            
    imageL = np.array(imageL, dtype = 'float32')
    labelL = np.array(labelL, dtype = 'int32') 
    output.append((imageL, labelL))
    #print(len(output[0]))
    #print(len(imageL))
    #print(len(labelL))
    #print(labelL[0])
    #plt.imshow(imageL[0]/255)
  return output

#Load Data:
[(train_images, train_labels), (test_images, test_labels)] = load_data()

# Shuffle Train Dataset:
train_images, train_labels = shuffle(train_images, train_labels, random_state=20)

# Scale the data
train_images = train_images/255.0
test_images = test_images/255.0

"""### Explore the Dataset"""

print("Number of training images: ", len(train_images))
print("Number of training labels: ", len(train_labels))
print("Number of testing images: ", len(test_images))
print("Number of testing labels: ", len(test_labels))
print("Shape of images: ", train_images.shape)

def display_random_image(class_names, images, labels):
    randomIndex = np.random.randint(images.shape[0])
    plt.figure()
    plt.imshow(images[randomIndex])
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.title('Image {}: '.format(randomIndex) + class_names[labels[randomIndex]])
    plt.show()
    return;

display_random_image(class_names, train_images, train_labels)

def display_images(class_names, images, labels):
    fig = plt.figure(figsize=(10,10))
    for i in range(20):
        plt.subplot(5,5,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]])
    plt.show()
display_images(class_names,train_images,train_labels)

trainLabels_values, trainLabels_counts = np.unique(train_labels, return_counts=True)
testLabels_values, testLabels_counts = np.unique(test_labels, return_counts=True)

plt.pie(trainLabels_counts,
        explode=(.05, .05, .05, .05, .05, .05) , 
        labels=class_names,
        autopct='%1.1f%%')
plt.axis('equal')
plt.title('Proportion of each category')
plt.show()

"""### Build the model"""

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', padding='same', input_shape = (150, 150, 3)), 
    tf.keras.layers.MaxPooling2D((2,2), padding='same'),
    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2,2), padding='same'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(6, activation=tf.nn.softmax)
])

model.compile(optimizer = 'adam', 
              loss = 'sparse_categorical_crossentropy', 
              metrics=['accuracy'])

history = model.fit(train_images, 
                    train_labels, 
                    batch_size=128, 
                    epochs=20, 
                    validation_split = 0.2)

test = model.evaluate(test_images, test_labels)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

predictions = model.predict(test_images)     # Vector of probabilities
pred_labels = np.argmax(predictions, axis = 1) # We take the highest probability

def plot_prediction_image(predictions_array, predictions_labels, true_label, images):
    randomIndex = np.random.randint(images.shape[0])
    plt.imshow(images[randomIndex], cmap=plt.cm.binary)
    if predictions_labels[randomIndex] == true_label[randomIndex]:
        color = "blue"
    else:
        color = "red"
    plt.xlabel("objet pr√©dit {} {:2.0f}% ({})".format(class_names[predictions_labels[randomIndex]], np.max(predictions_array[randomIndex])*100, class_names[true_label[randomIndex]]), color=color)
    return

plot_prediction_image(predictions,pred_labels,test_labels,test_images)

from joblib import Parallel, delayed
import joblib
  
  
# Save the model as a pickle in a file
joblib.dump(model, 'modelClassification.pkl')
  
# Load the model from the file
model_saved = joblib.load('modelClassification.pkl')
  
# Use the loaded model to make predictions
model_saved.predict(test_images)